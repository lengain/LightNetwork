---
title: 大模型基础概念
date: 2025-11-25 22:09:16
tags: AI大模型
categories: 
---

# 大模型基础概念（适合客户端工程师的轻量入门）

下面内容专为客户端工程师写，不讲晦涩的数学，而是告诉你：

👉 **它是什么**  
👉 **为什么重要**  
👉 **你在实际 AI 应用中怎么用**

---

# Token / Context / Prompt / System Prompt

---

## 1. Token 是什么？

**Token 是模型处理文本的最小单位，可以是字、词或词片段。**

- GPT 类模型：英文按词片段、中文按字切分

- Token ≠ 字符

- 模型所有计算都按 token 进行

📌 **为什么 iOS 工程师必须懂 Token？**

- 调用 API 时会计费（按 token）

- 上下文长度（context window）限制以 token 计

- 多轮对话需要控制 token 数量，否则会 “爆 context”

例子：  
“我喜欢 iOS 开发”  
→ Token 可能被切成：  
["我", "喜欢", "i", "OS", "开发"]

<!-- more -->

---

## 2. Context（上下文）

**Context 是模型在本次推理中可记住的最大 token 数。**

例如：

| 模型          | 最大 Context |
| ----------- | ---------- |
| Qwen2       | 128k       |
| DeepSeek V3 | 128k       |
| GPT-4 Turbo | 128k       |

📌 为什么要在工程侧特别关注？

- 多轮对话越长，越容易“爆 context”

- “爆 context”模型会忘记前面内容

- 所以你需要 **对历史对话做裁剪 / 精简 / 只保留重要内容**

你的 iOS Demo 里需要实现：

### 👉 **对话上下文管理模块**

- 限制最大 token

- 自动删除最早的对话

- 或做摘要（summary）

---

## 3. Prompt / System Prompt

**Prompt = 让模型做事的指令文本。**

### Prompt 三种角色

1. **System prompt**
   
   - 定义模型角色
   
   - 决定模型长期行为
   
   - 整个对话只需要设一次  
     例如：  
     “你是一名 iOS 工程师助手，回答尽量简洁。”

2. **User prompt**  
   用户输入内容

3. **Assistant prompt**  
   模型回复内容

---

### System Prompt 的作用（非常重要）

它相当于 **系统级配置**，影响整个对话：

- 输出格式（JSON、Markdown）

- 模型人格（严谨、简短、幽默）

- 禁止幻觉（必须引用事实）

- 绑定工具（function calling）

### 📌 在 iOS 中用法

通常会是这样：

```swift
let systemPrompt = """
你是一名专业的 iOS AI 助手。
所有回答必须使用简洁语言，如果用户要求代码，请使用 Swift。
"""
```

然后在请求 body 中设置：

```json
"messages": [
    { "role": "system", "content": systemPrompt },
    { "role": "user", "content": userInput }
]
```

---

# Function Calling（工具调用）

这是做AI应用的核心能力。

---

## Function Calling 是什么？

**Function Calling = 让 LLM 选择并调用某个你提供的函数。**

举例：

你告诉模型：

> “如果用户说 ‘查天气’，请调用我提供的 `getWeather(city: String)` 函数。”

模型不会真的执行代码，它会返回 JSON：

```json
{
  "name": "getWeather",
  "arguments": {
    "city": "上海"
  }
}
```

iOS 侧解析 JSON → 调用真实 Swift 函数 → 把结果再传回模型。

---

## ⭐ 为什么这是 AI 应用工程师的核心能力？

- 让模型控制你的 App（提醒、查快递、查天气…）

- 让模型具备“能力”，而不是只能聊天

- 让你的 App 有“可组合 AI 行为”

这是支付宝、微信 AI 团队面试最喜欢问的部分。

---

## Function Calling 的使用流程（工程侧）

### ① 定义 functions 数组（描述可调用的工具）

```json
"functions": [
  {
    "name": "getWeather",
    "description": "获取城市天气",
    "parameters": {
      "type": "object",
      "properties": {
        "city": { "type": "string" }
      },
      "required": ["city"]
    }
  }
]
```

### ② 模型返回对应 JSON（告诉我们要调哪一个函数）

### ③ iOS 侧解析 JSON → 调用 Swift 函数

### ④ 把结果再发给模型，让模型生成自然语言

例如智能卡片、智能助手完全基于此能力实现。

---

# Embedding（向量化）及其用法

这是后续 RAG、意图识别、分类、召回 的核心。

---

## Embedding 是什么？

**Embedding = 把一句话转成一串数字向量（如 768 维）。**

例：

“查快递”  
→ [0.11, -0.23, 0.08, ...]（768 个数）

模型把语义含义编码到这些数字里。

---

## 为什么 Embedding 如此重要？

因为 **LLM 只能做“生成”，不能做“检索”。**  
真正的工程应用想要：

- 意图识别

- 文档搜索

- 相似度匹配

- 知识库查询

- 工具选择

→ **全部要用 embedding。**

---

# Embedding 的三个核心用途

---

## ① 文档搜索（RAG 核心）

流程：

1. 把 PDF 切成小段

2. 每段转成 embedding

3. 用户问题 → embedding

4. 计算相似度

5. 找到最接近的 3–5 段

6. 和用户问题一起喂给 LLM → 回答

你的“iOS-RAG-Demo”就是这样。

---

## ② 意图识别（智能卡片核心）

把常见意图 embedding 成向量：

| 意图类别 | 示例         |
| ---- | ---------- |
| 查快递  | 查一下快递…     |
| 查天气  | 今天上海天气     |
| 设置提醒 | 明天 8 点叫我起床 |

用户输入 → embedding  
与各意图比相似度 → 找到最接近的 → 触发卡片

这是“iOS-AI-Card-Assistant”的关键技术。

---

## ③ 工具选择（function calling 自动化）

通过 embedding 判断用户是在提问还是要调用工具。

避免“你可能想…”这种幻觉。

---

## 📌 在 iOS 中调用 Embedding API

通常是调用：

- Qwen Embedding API（推荐）

- GLM Embedding

- OpenAI Embedding

请求 body 示意：

```json
{
  "model": "text-embedding",
  "input": "帮我查一下快递"
}
```

返回：

```json
{
  "embedding": [0.12, -0.03, ...]
}
```

然后存入 SQLite、CoreData，或者本地文件。

---

# 最后：三者之间的关系

```
Prompt —— 让模型“理解你”
Token —— 决定上下文大小与消耗
Context —— 模型能记住多少
Function Calling —— 让模型“操作你的 App”
Embedding —— 让应用“理解语义”并构建智能功能
```

**Prompt = 生成侧**  
**Embedding = 检索侧**  
**Function Calling = 执行侧**

三者结合 → 能做出真正高级的智能助手。
